{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path for local imports\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import bcolz\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu: True \n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print('Using gpu: %s ' % use_gpu)\n",
    "\n",
    "def gpu(x,use_gpu=use_gpu):\n",
    "    if use_gpu:\n",
    "        return x.cuda()\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = dset.ImageFolder('../Dataset/train', transform=transform)\n",
    "val_dataset = dset.ImageFolder('../Dataset/val', transform=transform)\n",
    "test_dataset = dset.ImageFolder('../Dataset/test', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_dataset)\n",
    "val_size = len(val_dataset)\n",
    "test_size = len(test_dataset)\n",
    "print(\"Number of training examples {}, validation examples {}, testing examples {}\".format(train_size, val_size, test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=6)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=6)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodEnsemble(nn.Module):\n",
    "    def __init__(self, resnet, densenet):\n",
    "        self.resnet = resnet\n",
    "        self.densenet = densenet\n",
    "        self.alpha = 0.5\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            a1 = self.resnet(x)\n",
    "            a2 = self.densenet(x)\n",
    "        return self.alpha * a1 + (1 - self.alpha) * a2\n",
    "        \n",
    "\n",
    "resnet = gpu(torch.load(\"../saved_models/food/res18/res18.dat\"))\n",
    "densenet = gpu(torch.load(\"../saved_models/food/densenet/densenet.dat\"))\n",
    "FoodEnsemble(resnet, densenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model):\n",
    "    num_correct, num_samples, total_loss = 0, 0, 0\n",
    "    model.eval()\n",
    "    batches = val_dataloader\n",
    "    with torch.no_grad():\n",
    "        for x, y in batches:\n",
    "            x, y = gpu(x), gpu(y)                \n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y) \n",
    "            _, preds = torch.max(scores.data, 1)\n",
    "            total_loss += loss.data.item()\n",
    "            num_correct += torch.sum(preds == y.data)\n",
    "            num_samples += preds.size(0)\n",
    "        average_loss = total_loss / num_samples\n",
    "        acc = num_correct / num_samples\n",
    "    print('Validation Loss: {:.4f} Got {} / {} correct {:.2f}%'.format(\n",
    "        average_loss, num_correct, num_samples, 100 * acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss: 0.0414 Got 27637 / 66071 correct. Acc: 41.83%\n",
      "Epoch: 1 Validation Loss: 0.0295 Got 5813 / 11016 correct 52.77%\n",
      "Epoch: 2 Training Loss: 0.0268 Got 37798 / 66071 correct. Acc: 57.21%\n",
      "Epoch: 2 Validation Loss: 0.0262 Got 6261 / 11016 correct 56.84%\n",
      "Epoch: 3 Training Loss: 0.0239 Got 40254 / 66071 correct. Acc: 60.93%\n",
      "Epoch: 3 Validation Loss: 0.0250 Got 6499 / 11016 correct 59.00%\n",
      "Epoch: 4 Training Loss: 0.0224 Got 41382 / 66071 correct. Acc: 62.63%\n",
      "Epoch: 4 Validation Loss: 0.0248 Got 6508 / 11016 correct 59.08%\n",
      "Epoch: 5 Training Loss: 0.0214 Got 42447 / 66071 correct. Acc: 64.24%\n",
      "Epoch: 5 Validation Loss: 0.0248 Got 6533 / 11016 correct 59.30%\n",
      "Epoch: 6 Training Loss: 0.0206 Got 43204 / 66071 correct. Acc: 65.39%\n",
      "Epoch: 6 Validation Loss: 0.0245 Got 6590 / 11016 correct 59.82%\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "check_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model.\n",
    "PATH = \"../saved_models/food/res18/res18.dat\"\n",
    "torch.save(model, PATH)\n",
    "torch.save({'loss': LOSS, 'acc': ACC, 'loss_v': LOSS_V, 'acc_v': ACC_V}, '../saved_models/food/res18/res18-history.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "PATH = \"../saved_models/food/res18/res18.dat\"\n",
    "loaded_model = torch.load(PATH)\n",
    "loaded_model = gpu(loaded_model)\n",
    "model = loaded_model\n",
    "history = torch.load('../saved_models/food/res18/res18-history.pt')\n",
    "LOSS, ACC, LOSS_V, ACC_V = (history['loss'], history['acc'], history['loss_v'], history['acc_v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Accuracy Curves\n",
    "from utils.plots import plot_loss_and_accuracy_curves\n",
    "plot_loss_and_accuracy_curves('ResNet18', LOSS, ACC, LOSS_V, ACC_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: Top-1 Accuracy\n",
    "\n",
    "def compute_top_1_accuracy(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in test_dataloader:\n",
    "            images, labels = data\n",
    "            images = gpu(images)\n",
    "            labels = gpu(labels)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Top 1 Accuracy of the network on the all test images: %.2f %%' % (\n",
    "        100 * correct / total))\n",
    "\n",
    "compute_top_1_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: Top-5 Accuracy\n",
    "\n",
    "def compute_top_5_accuracy(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in test_dataloader:\n",
    "            images, labels = data\n",
    "            images = gpu(images)\n",
    "            labels = gpu(labels)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.topk(outputs.data, 5, dim=1)\n",
    "            total += labels.size(0)\n",
    "            for i in range(predicted.shape[0]):\n",
    "                top_5_predictions = predicted[i]\n",
    "                label = labels[i]\n",
    "                if label in top_5_predictions:\n",
    "                    correct += 1\n",
    "    print('Top 5 Accuracy of the network on the all test images: %.2f %%' % (\n",
    "        100 * correct / total))\n",
    "    \n",
    "compute_top_5_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store y_pred and y_test on the test set for evaluation.\n",
    "\n",
    "y_pred = []\n",
    "y_test = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        images = gpu(images)\n",
    "        labels = gpu(labels)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy().tolist())\n",
    "        y_test.extend(labels.cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix\\n')\n",
    "print(cnf_matrix)\n",
    "print(cnf_matrix[0][0], sum(cnf_matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the confusion matrix \n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "array = cnf_matrix.tolist()\n",
    "df_cm = pd.DataFrame(array, index = [i for i in range(172)],\n",
    "                  columns = [i for i in range(172)])\n",
    "plt.figure(figsize = (100,100))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: Compute several statistics such as accuracy, precision, F1-score, and produce a classification report\n",
    "\n",
    "FOOD_LIST_PATH = '../VireoFood172/SplitAndIngreLabel/FoodList.txt'\n",
    "food_names = [] # List of length 172 where index is the food label, and value is the food name.\n",
    "with open(FOOD_LIST_PATH) as fp:\n",
    "    food_names = fp.read().splitlines()\n",
    "    \n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test, y_pred, target_names=food_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class 0 Statistics\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "for pred,label in zip(y_pred, y_test):\n",
    "    if pred == 0 and label == 0:\n",
    "        TP += 1\n",
    "    if pred == 0 and label != 0:\n",
    "        FP += 1\n",
    "    if pred != 0 and label == 0:\n",
    "        FN += 1\n",
    "print(\"Class 0 ({}) Statistics\".format(food_names[0]))\n",
    "print(\"True Positives: {}\".format(TP))\n",
    "print(\"False Positives: {}\".format(FP))\n",
    "print(\"False Negatives: {}\".format(FN))\n",
    "print(\"Precision: {}\".format(TP / (TP + FP)))\n",
    "print(\"Recall: {}\".format(TP / (TP + FN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
